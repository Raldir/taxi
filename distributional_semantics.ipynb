{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import os\n",
    "\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "from chinese_whispers import chinese_whispers, aggregate_clusters\n",
    "from gensim.models.poincare import PoincareModel\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the Networkx graph\n",
    "From a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_taxonomy(graph):\n",
    "    \"\"\" Display the taxonomy in a hierarchical layout \"\"\"\n",
    "    pos = graphviz_layout(graph, prog='dot', args=\"-Grankdir=LR\")\n",
    "    plt.figure(3,figsize=(48,144))\n",
    "    nx.draw(graph, pos, with_labels=True, arrows=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'taxi_output/simple_full/science/science_en.csv-relations.csv-taxo-knn1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the taxonomy as a dataframe\n",
    "df = pd.read_csv(\n",
    "    input_path,\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=['hyponym', 'hypernym'],\n",
    "    usecols=[1,2],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the networkx graph\n",
    "G = nx.DiGraph()\n",
    "for rel in zip(list(df['hypernym']), list(df['hyponym'])):\n",
    "    \n",
    "    rel_0 = rel[0]\n",
    "    rel_1 = rel[1]\n",
    "    \n",
    "    # Simplify the compound words by replacing the whitespaces with underscores\n",
    "    if ' ' in rel[0]:\n",
    "        rel_0 = '_'.join(rel[0].split())\n",
    "    if ' ' in rel[1]:\n",
    "        rel_1 = '_'.join(rel[1].split())\n",
    "    G.add_edge(rel_0, rel_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nx.isolates(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors():\n",
    "    \"\"\" Load word vectors. \"\"\"\n",
    "\n",
    "    embedding_dir = '/home/5aly/taxi/distributed_semantics/embeddings/'\n",
    "\n",
    "    poincare_model = model = PoincareModel.load(embedding_dir + 'embeddings_poincare_wordnet')  # parent-cluster relationship\n",
    "    own_model = gensim.models.KeyedVectors.load(embedding_dir + 'own_embeddings_w2v')  # family-cluster relationship\n",
    "\n",
    "    return poincare_model, own_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "poincare_w2v, own_w2v = load_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acharya/anaconda3/envs/tax3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/acharya/anaconda3/envs/tax3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('flatbread', 0.7123582363128662),\n",
       " ('breads', 0.7097251415252686),\n",
       " ('unleavened', 0.7093826532363892),\n",
       " ('gruel', 0.700811505317688),\n",
       " ('dough', 0.6893404722213745),\n",
       " ('flour', 0.6792106628417969),\n",
       " ('matzo', 0.6730862855911255),\n",
       " ('loaves', 0.6616495847702026),\n",
       " ('curd', 0.6615822315216064),\n",
       " ('cheese', 0.6587767601013184)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "own_w2v.most_similar('bread')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Taxonomy with Distributional Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a networkx graph for each node containing only its children. Draw edges among the children based on the similarity with one another using word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_children_clusters(own_model, graph):\n",
    "    \"\"\" This function returns a dictionary where corresponding to each key(node) is a graph of its children \"\"\"\n",
    "    \n",
    "    clustered_graph = {}\n",
    "    for node in graph.nodes():\n",
    "        clustered_graph[node] = nx.Graph()\n",
    "        successors = [s.lower() for s in graph.successors(node)]\n",
    "\n",
    "        for successor in successors:\n",
    "            try:\n",
    "                for word, _ in own_model.most_similar(successor, topn=100):\n",
    "                    if word.lower() in successors:\n",
    "                        clustered_graph[node].add_edge(successor, word.lower())\n",
    "            except KeyError:  # If the word in not in vocabulary, check using the substring based method\n",
    "                successor_terms = successor.split('_')\n",
    "                if node in successor_terms:\n",
    "                    clustered_graph[node].add_node(successor)\n",
    "    \n",
    "    return clustered_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GC = create_children_clusters(own_w2v, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posI = graphviz_layout(GC['engineering'])\n",
    "# plt.figure(2, figsize=(20, 20))\n",
    "nx.draw(GC['engineering'], posI, with_labels=True, arrows=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Chinese Whispers Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of smaller clusters\n",
    "- For every node, cluster its children.\n",
    "- Keep only the biggest cluster and detach the rest from the graph.  \n",
    "- Store the removed clusters in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_clusters(own_model, nx_graph):\n",
    "    \"\"\" Removes the less related and small clusters from the graph \"\"\"\n",
    "\n",
    "    print('Removing small clusters..')\n",
    "    g_clustered = create_children_clusters(own_model, nx_graph)\n",
    "    removed_clusters = []\n",
    "\n",
    "    nodes, clusters, size_ratio = [], [], []\n",
    "    for node, graph in g_clustered.items():\n",
    "        gc = chinese_whispers(graph, weighting='top', iterations=60)\n",
    "        try:  # Get the length of the largest cluster\n",
    "            max_cluster_size = len(max(aggregate_clusters(gc).values(), key=len))\n",
    "        except ValueError:\n",
    "            continue\n",
    "        \n",
    "        # Calculate the size ratio of all the clusters which are smaller than the largest\n",
    "        for _, cluster in aggregate_clusters(gc).items():\n",
    "            if len(cluster) < max_cluster_size:\n",
    "                nodes.append(node)\n",
    "                clusters.append(cluster)\n",
    "                size_ratio.append(len(cluster) / max_cluster_size)\n",
    "    \n",
    "    # Sort the small clusters according to their size_ratio\n",
    "    sorted_node_clusters = [(node, cluster) for _, cluster, node in sorted(zip(size_ratio, clusters, nodes))]\n",
    "    if len(sorted_node_clusters) > 10:\n",
    "        sorted_node_clusters = sorted_node_clusters[:10]\n",
    "\n",
    "    for node, cluster in sorted_node_clusters:  # detach only the smallest 10 clusters in the entire taxonomy\n",
    "        removed_clusters.append(cluster)\n",
    "        for item in cluster:\n",
    "            nx_graph.remove_edge(node, item)\n",
    "\n",
    "    return nx_graph, removed_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_improved = G.copy()\n",
    "G_improved, removed_clusters = remove_clusters(own_w2v, G_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(removed_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding back the removed clusters\n",
    "- Loop through all the removed clusters.\n",
    "- For each removed cluster, find out the cluster in the graph that has the maximum similarity with it.\n",
    "\n",
    "Similarity between two clusters is computed by calculating the average of the pairwise similarity of the elements of both the clusters i.e. NxM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GC_detached = create_children_clusters(own_w2v, G_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(poincare_model, own_model, parent, family, cluster, exclude_parent, exclude_family):\n",
    "    \n",
    "    # Similarity between the parent and a cluster\n",
    "    parent_similarity = 0\n",
    "    if not exclude_parent:\n",
    "        parent_similarities = []\n",
    "        for item in cluster:\n",
    "            max_similarity = 0\n",
    "            item_senses = [i_sense.name() for i_sense in wn.synsets(item) if item in i_sense.name()]\n",
    "            parent_senses = [p_sense.name() for p_sense in wn.synsets(parent) if parent in p_sense.name()]\n",
    "            for parent_sense in parent_senses:\n",
    "                for item_sense in item_senses:\n",
    "                    try:\n",
    "                        similarity = poincare_model.kv.similarity(parent_sense, item_sense)\n",
    "                        if similarity > max_similarity:\n",
    "                            max_similarity = similarity\n",
    "                    except KeyError as e:\n",
    "                        if parent_sense.name() in str(e):\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "            if max_similarity != 0:\n",
    "                parent_similarities.append(max_similarity)\n",
    "        if len(parent_similarities) > 0:  # Happens when the cluster has only one item which is not in vocabulary\n",
    "            parent_similarity = sum(parent_similarities) / len(parent_similarities)\n",
    "    \n",
    "    # Similarity between a family and a cluster\n",
    "    family_similarity = 0\n",
    "    if not exclude_family:\n",
    "        family_similarities = []\n",
    "        for f_item in family:\n",
    "            for c_item in cluster:\n",
    "                try:\n",
    "                    family_similarities.append(own_model.similarity(f_item, c_item))\n",
    "                except KeyError as e:  # skip the terms not in vocabulary\n",
    "                    if f_item in str(e):\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "        if len(family_similarities) > 0:\n",
    "            family_similarity = sum(family_similarities) / len(family_similarities)\n",
    "    \n",
    "    # Final score is the average of both the similarities\n",
    "    return (parent_similarity + family_similarity) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in removed_clusters:\n",
    "    max_score = 0\n",
    "    max_score_node = ''\n",
    "    for node, graph in GC_detached.items():\n",
    "        gc = chinese_whispers(graph, weighting='top', iterations=60)\n",
    "        for label, family in aggregate_clusters(gc).items():\n",
    "            score = calculate_similarity(poincare_w2v, own_w2v, node, family, cluster, False, False)\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                max_score_node = node\n",
    "    for item in cluster:\n",
    "        G_improved.add_edge(max_score_node, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the nodes and the edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_result(g_improved):\n",
    "    \"\"\" Filter the results i.e. remove all the isolated nodes and nodes with blank labels \"\"\"\n",
    "\n",
    "    print('\\nTuning the result...')\n",
    "\n",
    "    if '' in g_improved.nodes():\n",
    "        g_improved.remove_node('')\n",
    "\n",
    "    hypernyms = {x[0] for x in g_improved.edges()}\n",
    "    isolated_nodes = list(nx.isolates(g_improved))\n",
    "    for isolated_node in isolated_nodes:\n",
    "        terms = isolated_node.split('_')\n",
    "        if terms[-1] in hypernyms:\n",
    "            g_improved.add_edge(terms[-1], isolated_node)\n",
    "        elif terms[0] in hypernyms:\n",
    "            g_improved.add_edge(terms[0], isolated_node)\n",
    "        else:\n",
    "            g_improved.remove_node(isolated_node)\n",
    "\n",
    "    return g_improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_result(G_improved)\n",
    "print('Tuned.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nx.isolates(G_improved))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line_count(file_name):\n",
    "    \"\"\" Counts the number of lines in a file using bash \"\"\"\n",
    "\n",
    "    return int(subprocess.check_output(\n",
    "        \"wc -l {file_name} | grep -o -E '^[0-9]+'\".format(file_name=file_name), shell=True\n",
    "    ).decode('utf-8').split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_pruning(path, output_dir, domain):\n",
    "\n",
    "    cycle_removing_tool = \"graph_pruning/graph_pruning.py\"\n",
    "    cycle_removing_method = \"tarjan\"\n",
    "    cleaning_tool = \"graph_pruning/cleaning.py\"\n",
    "\n",
    "    input_file = os.path.basename(path)\n",
    "    file_pruned_out = input_file + '-pruned.csv'\n",
    "    file_cleaned_out = file_pruned_out + '-cleaned.csv'\n",
    "\n",
    "    print('======================================================================================================================')\n",
    "    cycle_removing_cmd = 'python {cycle_removing_tool} {input_path} {output_dir}/{file_pruned_out} {cycle_removing_method} | tee /dev/tty'.format(\n",
    "        cycle_removing_tool=cycle_removing_tool,\n",
    "        input_path=path,\n",
    "        output_dir=output_dir,\n",
    "        file_pruned_out=file_pruned_out,\n",
    "        cycle_removing_method=cycle_removing_method\n",
    "    )\n",
    "    print('Cycle removing:', cycle_removing_cmd, sep='\\n')\n",
    "    subprocess.check_output(cycle_removing_cmd, shell=True)\n",
    "    print('Cycle removing finished. Written to: {output_dir}/{file_pruned_out}\\n'.format(\n",
    "        output_dir=output_dir, file_pruned_out=file_pruned_out\n",
    "    ))\n",
    "\n",
    "    print('======================================================================================================================')\n",
    "    cleaning_cmd = 'python {cleaning_tool} {output_dir}/{file_pruned_out} {output_dir}/{file_cleaned_out} {domain}'.format(\n",
    "        cleaning_tool=cleaning_tool,\n",
    "        output_dir=output_dir,\n",
    "        file_pruned_out=file_pruned_out,\n",
    "        file_cleaned_out=file_cleaned_out,\n",
    "        domain=domain\n",
    "    )\n",
    "    print('Cleaning:', cleaning_cmd, sep='\\n')\n",
    "    subprocess.check_output(cleaning_cmd, shell=True)\n",
    "    print('Finished cleaning. Write output to: {output_dir}/{file_cleaned_out}\\n'.format(\n",
    "        output_dir=output_dir, file_cleaned_out=file_cleaned_out\n",
    "    ))\n",
    "\n",
    "    return os.path.join(output_dir, file_cleaned_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(system_generated_taxo, output_dir, domain):\n",
    "    \"\"\" Calculate the F1 score of the re-generated taxonomies \"\"\"\n",
    "\n",
    "    eval_tool = 'eval/taxi_eval_archive/TExEval.jar'\n",
    "    eval_gold_standard = 'eval/taxi_eval_archive/gold_standard/{domain}.taxo'.format(domain=domain)\n",
    "    eval_root = domain\n",
    "    eval_jvm = '-Xmx9000m'\n",
    "    eval_tool_result = os.path.join(output_dir, os.path.basename(system_generated_taxo)) + '-evalresult.txt'\n",
    "\n",
    "    # Running the tool\n",
    "    print('======================================================================================================================')\n",
    "    tool_command = \"\"\"java {eval_jvm} -jar {eval_tool} {system_generated_taxo} {eval_gold_standard} {eval_root} {eval_tool_result}\"\"\".format(\n",
    "        eval_jvm=eval_jvm,\n",
    "        eval_tool=eval_tool,\n",
    "        system_generated_taxo=system_generated_taxo,\n",
    "        eval_gold_standard=eval_gold_standard,\n",
    "        eval_root=eval_root,\n",
    "        eval_tool_result=eval_tool_result\n",
    "    )\n",
    "    print('Running eval-tool:', tool_command, sep='\\n')\n",
    "    subprocess.check_output(tool_command, shell=True)\n",
    "    print('Result of eval-tool written to:', eval_tool_result)\n",
    "\n",
    "    # Calculating Precision, F1 score and F&M Measure\n",
    "    scores = {}\n",
    "    l_gold = get_line_count(eval_gold_standard)\n",
    "    l_input = get_line_count(system_generated_taxo)\n",
    "\n",
    "    scores['recall'] = float(subprocess.check_output(\n",
    "        r\"tail -n 1 {eval_tool_result} | grep -o -E '[0-9]+[\\.]?[0-9]*'\".format(\n",
    "            eval_tool_result=eval_tool_result\n",
    "        ), shell=True\n",
    "    ).decode('utf-8').split('\\n')[0])\n",
    "    scores['precision'] = scores['recall'] * l_gold / l_input\n",
    "\n",
    "    scores['f1'] = 2 * scores['recall'] * scores['precision'] / (scores['recall'] + scores['precision'])\n",
    "    scores['f_m'] = float(subprocess.check_output(\n",
    "        r\"cat {eval_tool_result} | grep -o -E 'Cumulative Measure.*' | grep -o -E '0\\.[0-9]+'\".format(\n",
    "            eval_tool_result=eval_tool_result\n",
    "        ), shell=True\n",
    "    ).decode('utf-8').split('\\n')[0])\n",
    "\n",
    "    # Display results\n",
    "    print('\\nPrecision:', scores['precision'])\n",
    "    print('Recall:', scores['recall'])\n",
    "    print('F1:', scores['f1'])\n",
    "    print('F&M:', scores['f_m'])\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(result, path, mode):\n",
    "    print('\\nSaving the result...')\n",
    "    df_improved = pd.DataFrame(list(result.edges()), columns=['hypernym', 'hyponym'])\n",
    "    df_improved = df_improved[df_improved.columns.tolist()[::-1]]\n",
    "\n",
    "    # Replace the underscores with blanks\n",
    "    df_improved['hyponym'] = df_improved['hyponym'].apply(lambda x: x.replace('_', ' '))\n",
    "    df_improved['hypernym'] = df_improved['hypernym'].apply(lambda x: x.replace('_', ' '))\n",
    "\n",
    "    # Store the result\n",
    "    output_path = os.path.join(\n",
    "        'taxi_output', 'distributional_semantics',\n",
    "        os.path.basename(path) + '-' + mode + os.path.splitext(path)[-1]\n",
    "    )\n",
    "    df_improved.to_csv(output_path, sep='\\t', header=False)\n",
    "    print('Output saved at:', output_path)\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = save_result(G_improved, input_path, 'reattach')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_output = graph_pruning(output_path, 'out', 'science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = calculate_f1_score(pruned_output, 'out', 'science')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(graph):\n",
    "    \"\"\" Clusterize the nodes of a particular domain in a given graph \"\"\"\n",
    "    graph_cluster = chinese_whispers(graph, weighting='top', iterations=60)\n",
    "    \n",
    "    # Visualize the clustering of graph_cluster using NetworkX (requires matplotlib)\n",
    "    colors = [1. / graph_cluster.node[node]['label'] for node in graph_cluster.nodes()]\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(20, 20)\n",
    "    nx.draw_networkx(graph_cluster, cmap=plt.get_cmap('jet'), node_color=colors, font_color='black')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GC_improved = create_children_clusters(own_w2v, G_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'mechanical_engineering'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original clusters\n",
    "visualize_clusters(GC[domain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clusters after detaching\n",
    "visualize_clusters(GC_detached[domain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clusters after detaching and re-attaching the clusters\n",
    "visualize_clusters(GC_improved[domain])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the original taxonomy\n",
    "display_taxonomy(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the modified taxonomy\n",
    "display_taxonomy(G_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(G.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(G_improved.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
