{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import os\n",
    "\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "from chinese_whispers import chinese_whispers, aggregate_clusters\n",
    "from gensim.models.poincare import PoincareModel\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the Networkx graph\n",
    "From a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_taxonomy(graph):\n",
    "    \"\"\" Display the taxonomy in a hierarchical layout \"\"\"\n",
    "    pos = graphviz_layout(graph, prog='dot', args=\"-Grankdir=LR\")\n",
    "    plt.figure(3,figsize=(48,144))\n",
    "    nx.draw(graph, pos, with_labels=True, arrows=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../taxi_output/simple_full/science_en.csv-relations.csv-taxo-knn1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the taxonomy as a dataframe\n",
    "df = pd.read_csv(\n",
    "    '../taxi_output/simple_full/science_en.csv-relations.csv-taxo-knn1.csv',\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=['hyponym', 'hypernym'],\n",
    "    usecols=[1,2],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the networkx graph\n",
    "G = nx.DiGraph()\n",
    "for rel in zip(list(df['hypernym']), list(df['hyponym'])):\n",
    "    \n",
    "    rel_0 = rel[0]\n",
    "    rel_1 = rel[1]\n",
    "    \n",
    "    # Simplify the compound words by replacing the whitespaces with underscores\n",
    "    if ' ' in rel[0]:\n",
    "        rel_0 = '_'.join(rel[0].split())\n",
    "    if ' ' in rel[1]:\n",
    "        rel_1 = '_'.join(rel[1].split())\n",
    "    G.add_edge(rel_0, rel_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Word Vectors in gensim\n",
    "\n",
    "### If the pre-trained vectors are in '.vec' format, save them in a binary file\n",
    "This needs to be done only once:  \n",
    "Load the pre-trained vectors in **'.vec'** format and then save it in **'.bin'**, so that the loading of vectors is done quickly from next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors_old(path, mode='own', save_binary=False):\n",
    "    \"\"\" Load word vectors.\n",
    "        Mode Types:\n",
    "            - 'fast': Load word vectors from pre-trained embeddings in FastText\n",
    "            - 'own': Load word vectors from own embeddings\n",
    "        \n",
    "        To save the loaded vectors in binary format, set 'save_binary' to True\n",
    "    \"\"\"\n",
    "    \n",
    "    if mode == 'own':\n",
    "        model = gensim.models.KeyedVectors.load(path)\n",
    "    else:\n",
    "        if os.path.splitext(path)[-1] == '.vec':  # for pre-trained vectors in '.vec' format\n",
    "            model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=False, unicode_errors='ignore')\n",
    "            if save_binary:\n",
    "                model.save_word2vec_format(os.path.splitext(path)[0] + '.bin', binary=True)\n",
    "        else:  # for pre-trained vectors in '.bin' format\n",
    "            model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True, unicode_errors='ignore')\n",
    "        model.init_sims(replace=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(embedding):\n",
    "    \"\"\" Load word vectors. \"\"\"\n",
    "\n",
    "    embedding_dir = '/home/5aly/taxi/distributed_semantics/embeddings/'\n",
    "\n",
    "    if embedding == \"wiki2M\":\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format(embedding_dir + 'crawl-300d-2M.vec', binary=False)\n",
    "    elif embedding == \"wiki1M_subword\":\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "            embedding_dir + 'wiki-news-300d-1M-subword.vec', binary=False\n",
    "        )\n",
    "    elif embedding == \"own_w2v\":\n",
    "        model = gensim.models.KeyedVectors.load(embedding_dir + 'own_embeddings_w2v')\n",
    "    elif embedding == \"poincare\":\n",
    "        model = PoincareModel.load(embedding_dir + 'embeddings_poincare_wordnet')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = 'poincare'\n",
    "w2v = load_vectors(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Taxonomy with Distributional Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a networkx graph for each node containing only its children. Draw edges among the children based on the similarity with one another using word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_children_clusters(w2v_model, graph, embedding, depth=100):\n",
    "    \"\"\" This function returns a dictionary where corresponding to each key(node) is a graph of its children \"\"\"\n",
    "    clustered_graph = {}\n",
    "    for node in graph.nodes():\n",
    "        clustered_graph[node] = nx.Graph()\n",
    "        successors = [s.lower() for s in graph.successors(node)]\n",
    "\n",
    "        for successor in successors:\n",
    "            word_in_vocab = False\n",
    "            if embedding == \"poincare\":\n",
    "                word_senses = wn.synsets(successor)  # Get all the senses of the given node\n",
    "                for sense in word_senses:\n",
    "                    try:\n",
    "                        for word, score in w2v_model.kv.most_similar(sense.name(), topn=depth):\n",
    "                            word = word.split('.')[0]  # convert the word from poincare format to normal string\n",
    "                            word_in_vocab = True\n",
    "                            if word.lower() in successors:\n",
    "                                clustered_graph[node].add_edge(successor, word.lower())\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "            else:\n",
    "                try:\n",
    "                    for word, score in w2v_model.most_similar(successor, topn=depth):\n",
    "                        word_in_vocab = True\n",
    "                        if word.lower() in successors:\n",
    "                            clustered_graph[node].add_edge(successor, word.lower())\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            \n",
    "            if not word_in_vocab:  # If the word in not in vocabulary, check using the substring based method\n",
    "                successor_terms = successor.split('_')\n",
    "                root_terms = [successor_terms[0], successor_terms[-1]]\n",
    "                if node in root_terms:\n",
    "                    clustered_graph[node].add_node(successor)\n",
    "    \n",
    "    return clustered_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GC = create_children_clusters(w2v, G, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posI = graphviz_layout(GC['engineering'])\n",
    "# plt.figure(2, figsize=(20, 20))\n",
    "nx.draw(GC['engineering'], posI, with_labels=True, arrows=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Chinese Whispers Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of smaller clusters\n",
    "- For every node, cluster its children.\n",
    "- Keep only the biggest cluster and detach the rest from the graph.  \n",
    "- Store the removed clusters in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_clusters(model, nx_graph, embedding, depth=100):\n",
    "    \"\"\" Removes the less related and small clusters from the graph \"\"\"\n",
    "\n",
    "    print('Removing small clusters..')\n",
    "    g_clustered = create_children_clusters(model, nx_graph, embedding, depth)\n",
    "    removed_clusters = []\n",
    "\n",
    "    for node, graph in g_clustered.items():\n",
    "        gc = chinese_whispers(graph, weighting='top', iterations=60)\n",
    "        try:\n",
    "            max_cluster_size = len(max(aggregate_clusters(gc).values(), key=len))\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        clusters, size_ratio = [], []\n",
    "        for label, cluster in aggregate_clusters(gc).items():\n",
    "            clusters.append(cluster)\n",
    "            size_ratio.append(len(cluster) / max_cluster_size)\n",
    "\n",
    "        sorted_clusters = [cluster for _, cluster in sorted(zip(size_ratio, clusters))]\n",
    "        if len(sorted_clusters) > 10:\n",
    "            sorted_clusters = sorted_clusters[:10]\n",
    "\n",
    "        for cluster in sorted_clusters:  # detach smallest 10 clusters\n",
    "            removed_clusters.append(cluster)\n",
    "            for item in cluster:\n",
    "                nx_graph.remove_edge(node, item)\n",
    "\n",
    "    return nx_graph, removed_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_improved = G.copy()\n",
    "G_improved, removed_clusters = remove_clusters(w2v, G_improved, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(removed_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding back the removed clusters\n",
    "- Loop through all the removed clusters.\n",
    "- For each removed cluster, find out the cluster in the graph that has the maximum similarity with it.\n",
    "\n",
    "Similarity between two clusters is computed by calculating the average of the pairwise similarity of the elements of both the clusters i.e. NxM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(w2v_model, parent, family, cluster, embedding):\n",
    "    \n",
    "    # Similarity between the parent and a cluster\n",
    "    parent_similarity = 0\n",
    "    for item in cluster:\n",
    "        if embedding == \"poincare\":\n",
    "            item_senses = wn.synsets(item)\n",
    "            parent_senses = wn.synsets(parent)\n",
    "            for parent_sense in parent_senses:\n",
    "                for item_sense in item_senses:\n",
    "                    try:\n",
    "                        parent_similarity += w2v_model.kv.similarity(parent_sense, item_sense.name())\n",
    "                    except KeyError as e:\n",
    "                        if parent_sense in str(e):\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "        else:\n",
    "            try:\n",
    "                parent_similarity += w2v_model.similarity(parent, item)\n",
    "            except KeyError:  # skip the terms not in vocabulary\n",
    "                continue\n",
    "    parent_similarity /= len(cluster)\n",
    "    \n",
    "    # Similarity between a family and a cluster\n",
    "    family_similarity = 0\n",
    "    for f_item in family:\n",
    "        for c_item in cluster:\n",
    "            if embedding == \"poincare\":\n",
    "                f_senses = wn.synsets(f_item)\n",
    "                c_senses = wn.synsets(c_item)\n",
    "                for f_sense in f_senses:\n",
    "                    for c_sense in c_senses:\n",
    "                        try:\n",
    "                            family_similarity += w2v_model.kv.similarity(f_sense, c_sense)\n",
    "                        except KeyError as e:\n",
    "                            if f_sense in str(e):\n",
    "                                break\n",
    "                            else:\n",
    "                                continue\n",
    "            else:\n",
    "                try:\n",
    "                    family_similarity += w2v_model.similarity(f_item, c_item)\n",
    "                except KeyError:  # skip the terms not in vocabulary\n",
    "                    continue\n",
    "    family_similarity /= (len(family) * len(cluster))\n",
    "    \n",
    "    # Final score is the average of both the similarities\n",
    "    return (parent_similarity + family_similarity) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GC_detached = create_children_clusters(w2v, G_improved, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in removed_clusters:\n",
    "    max_score = 0\n",
    "    max_score_node = ''\n",
    "    for node, graph in GC_detached.items():\n",
    "        gc = chinese_whispers(graph, weighting='top', iterations=60)\n",
    "        for label, family in aggregate_clusters(gc).items():\n",
    "            score = calculate_similarity(w2v, node, family, cluster, embeddings)\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                max_score_node = node\n",
    "    for item in cluster:\n",
    "        G_improved.add_edge(max_score_node, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the nodes and the edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '' in G_improved.nodes():\n",
    "    G_improved.remove_node('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernyms = {x[0] for x in G_improved.edges()}\n",
    "isolated_nodes = list(nx.isolates(G_improved))\n",
    "\n",
    "for isolated_node in isolated_nodes:\n",
    "    terms = isolated_node.split('_')\n",
    "    if terms[-1] in hypernyms:\n",
    "        G_improved.add_edge(terms[-1], isolated_node)\n",
    "    elif terms[0] in hypernyms:\n",
    "        G_improved.add_edge(terms[0], isolated_node)\n",
    "    else:\n",
    "        G_improved.remove_node(isolated_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(graph):\n",
    "    \"\"\" Clusterize the nodes of a particular domain in a given graph \"\"\"\n",
    "    graph_cluster = chinese_whispers(graph, weighting='top', iterations=60)\n",
    "    \n",
    "    # Visualize the clustering of graph_cluster using NetworkX (requires matplotlib)\n",
    "    colors = [1. / graph_cluster.node[node]['label'] for node in graph_cluster.nodes()]\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(20, 20)\n",
    "    nx.draw_networkx(graph_cluster, cmap=plt.get_cmap('jet'), node_color=colors, font_color='black')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GC_improved = create_children_clusters(w2v, G_improved, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'engineering'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original clusters\n",
    "visualize_clusters(GC[domain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clusters after detaching\n",
    "visualize_clusters(GC_detached[domain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clusters after detaching and re-attaching the clusters\n",
    "visualize_clusters(GC_improved[domain])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the original taxonomy\n",
    "display_taxonomy(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the modified taxonomy\n",
    "display_taxonomy(G_improved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_improved = pd.DataFrame(list(G_improved.edges()), columns=['hypernym', 'hyponym'])\n",
    "df_improved = df_improved[df_improved.columns.tolist()[::-1]]\n",
    "\n",
    "# Replace the underscores with blanks\n",
    "df_improved['hyponym'] = df_improved['hyponym'].apply(lambda x: x.replace('_', ' '))\n",
    "df_improved['hypernym'] = df_improved['hypernym'].apply(lambda x: x.replace('_', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.splitext(input_path)\n",
    "output_path = '../taxi_output/distributional_semantics/' + file_path[0].split('/')[-1] + '-semantic-poincare' + file_path[1]\n",
    "df_improved.to_csv(output_path, sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
