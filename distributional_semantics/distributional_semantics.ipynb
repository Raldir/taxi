{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import os\n",
    "\n",
    "from networkx.drawing.nx_agraph import write_dot, graphviz_layout\n",
    "from chinese_whispers import chinese_whispers, aggregate_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the Networkx graph\n",
    "From a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_taxonomy(graph):\n",
    "    \"\"\" Display the taxonomy in a hierarchical layout \"\"\"\n",
    "    pos = graphviz_layout(G, prog='dot', args=\"-Grankdir=LR\")\n",
    "    plt.figure(3,figsize=(48,144))\n",
    "    nx.draw(G, pos, with_labels=True, arrows=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../taxi_output/simple_full/science_en.csv-relations.csv-taxo-knn1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the taxonomy as a dataframe\n",
    "df = pd.read_csv(\n",
    "    '../taxi_output/simple_full/science_en.csv-relations.csv-taxo-knn1.csv',\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=['hyponym', 'hypernym'],\n",
    "    usecols=[1,2],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the networkx graph\n",
    "G = nx.DiGraph()\n",
    "for rel in zip(list(df['hypernym']), list(df['hyponym'])):\n",
    "    \n",
    "    rel_0 = rel[0]\n",
    "    rel_1 = rel[1]\n",
    "    \n",
    "    # Simplify the compound words by replacing the whitespaces with underscores\n",
    "    if ' ' in rel[0]:\n",
    "        rel_0 = '_'.join(rel[0].split())\n",
    "    if ' ' in rel[1]:\n",
    "        rel_1 = '_'.join(rel[1].split())\n",
    "    G.add_edge(rel_0, rel_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Word Vectors in gensim\n",
    "\n",
    "### If the pre-trained vectors are in '.vec' format, save them in a binary file\n",
    "This needs to be done only once:  \n",
    "Load the pre-trained vectors in **'.vec'** format and then save it in **'.bin'**, so that the loading of vectors is done quickly from next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(path, mode='own', save_binary=False):\n",
    "    \"\"\" Load word vectors.\n",
    "        Mode Types:\n",
    "            - 'fast': Load word vectors from pre-trained embeddings in FastText\n",
    "            - 'own': Load word vectors from own embeddings\n",
    "        \n",
    "        To save the loaded vectors in binary format, set 'save_binary' to True\n",
    "    \"\"\"\n",
    "    \n",
    "    if mode == 'own':\n",
    "        model = gensim.models.KeyedVectors.load(path)\n",
    "    else:\n",
    "        if os.path.splitext(path)[-1] == '.vec':  # for pre-trained vectors in '.vec' format\n",
    "            model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=False, unicode_errors='ignore')\n",
    "            if save_binary:\n",
    "                w2v.save_word2vec_format(os.path.splitext(path)[0] + '.bin', binary=True)\n",
    "        else:  # for pre-trained vectors in '.bin' format\n",
    "            model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True, unicode_errors='ignore')\n",
    "        model.init_sims(replace=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = load_vectors('embeddings/own_embeddings_w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Taxonomy with Distributional Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a networkx graph for each node containing only its children. Draw edges among the children based on the similarity with one another using word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_children_clusters(graph):\n",
    "    \"\"\" This function returns a dictionary where corresponding to each key(node) is a graph of its children \"\"\"\n",
    "    clustered_graph = {}\n",
    "    for node in graph.nodes():\n",
    "        clustered_graph[node] = nx.Graph()\n",
    "        successors = [s.lower() for s in graph.successors(node)]\n",
    "\n",
    "        for successor in successors:\n",
    "            try:\n",
    "                for word, score in w2v.most_similar(successor):\n",
    "                    if word.lower() in successors:\n",
    "                        clustered_graph[node].add_edge(successor, word.lower())\n",
    "            except KeyError as e:\n",
    "                successor_terms = successor.split('_')\n",
    "                root_terms = [successor_terms[0], successor_terms[-1]]\n",
    "                if node in root_terms:\n",
    "                    clustered_graph[node].add_node(successor)\n",
    "    \n",
    "    return clustered_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acharya/anaconda3/envs/tax3/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/acharya/anaconda3/envs/tax3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "GC = create_children_clusters(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posI = graphviz_layout(GC['engineering'])\n",
    "# plt.figure(2, figsize=(20, 20))\n",
    "nx.draw(GC['engineering'], posI, with_labels=True, arrows=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Chinese Whispers Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of smaller clusters\n",
    "- For every node, cluster its children.\n",
    "- Keep only the biggest cluster and detach the rest from the graph.  \n",
    "- Store the removed clusters in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_improved = G.copy()\n",
    "removed_clusters = []\n",
    "\n",
    "for node, graph in GC.items():\n",
    "    gc = chinese_whispers(graph, weighting='top', iterations=60)\n",
    "    try:\n",
    "        max_cluster_size = len(max(aggregate_clusters(gc).values(), key=len))\n",
    "    except ValueError as e:\n",
    "        continue\n",
    "    for label, cluster in aggregate_clusters(gc).items():  # detach all the clusters smaller than the maximum\n",
    "        if len(cluster) < max_cluster_size:\n",
    "            removed_clusters.append(cluster)\n",
    "            for item in cluster:\n",
    "                G_improved.remove_edge(node, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(removed_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding back the removed clusters\n",
    "- Loop through all the removed clusters.\n",
    "- For each removed cluster, find out the cluster in the graph that has the maximum similarity with it.\n",
    "\n",
    "Similarity between two clusters is computed by calculating the average of the pairwise similarity of the elements of both the clusters i.e. NxM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cluster_similarity(cluster_1, cluster_2):\n",
    "    scores = []\n",
    "    for item_1 in cluster_1:\n",
    "        for item_2 in cluster_2:\n",
    "            try:\n",
    "                scores.append(w2v.similarity(item_1, item_2))\n",
    "            except KeyError as e:  # skip the terms not in vocabulary\n",
    "                continue\n",
    "    if len(scores) <= 0:\n",
    "        return 0\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acharya/anaconda3/envs/tax3/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/acharya/anaconda3/envs/tax3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "GC_detached = create_children_clusters(G_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in removed_clusters:\n",
    "    max_score = 0\n",
    "    max_score_node = ''\n",
    "    for node, graph in GC_detached.items():\n",
    "        gc = chinese_whispers(graph, weighting='top', iterations=60)\n",
    "        for label, cluster_new in aggregate_clusters(gc).items():\n",
    "            score = calculate_cluster_similarity(cluster, cluster_new)\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                max_score_node = node\n",
    "    for item in cluster:\n",
    "        G_improved.add_edge(max_score_node, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the nodes and the edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '' in G_improved.nodes():\n",
    "    G_improved.remove_node('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernyms = {x[0] for x in G_improved.edges()}\n",
    "isolated_nodes = list(nx.isolates(G_improved))\n",
    "\n",
    "for isolated_node in isolated_nodes:\n",
    "    terms = isolated_node.split('_')\n",
    "    if terms[-1] in hypernyms:\n",
    "        G_improved.add_edge(terms[-1], isolated_node)\n",
    "    elif terms[0] in hypernyms:\n",
    "        G_improved.add_edge(terms[0], isolated_node)\n",
    "    else:\n",
    "        G_improved.remove_node(isolated_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(graph):\n",
    "    \"\"\" Clusterize the nodes of a particular domain in a given graph \"\"\"\n",
    "    graph_cluster = chinese_whispers(graph, weighting='top', iterations=60)\n",
    "    \n",
    "    # Visualize the clustering of graph_cluster using NetworkX (requires matplotlib)\n",
    "    colors = [1. / graph_cluster.node[node]['label'] for node in graph_cluster.nodes()]\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(20, 20)\n",
    "    nx.draw_networkx(graph_cluster, cmap=plt.get_cmap('jet'), node_color=colors, font_color='black')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GC_improved = create_children_clusters(G_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'physics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original clusters\n",
    "visualize_clusters(GC[domain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clusters after detaching\n",
    "visualize_clusters(GC_detached[domain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clusters after detaching and re-attaching the clusters\n",
    "visualize_clusters(GC_improved[domain])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the original taxonomy\n",
    "display_taxonomy(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the modified taxonomy\n",
    "display_taxonomy(G_improved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_improved = pd.DataFrame(list(G_improved.edges()), columns=['hypernym', 'hyponym'])\n",
    "df_improved = df_improved[df_improved.columns.tolist()[::-1]]\n",
    "\n",
    "# Replace the underscores with blanks\n",
    "df_improved['hyponym'] = df_improved['hyponym'].apply(lambda x: x.replace('_', ' '))\n",
    "df_improved['hypernym'] = df_improved['hypernym'].apply(lambda x: x.replace('_', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.splitext(input_path)\n",
    "output_path = '../taxi_output/distributional_semantics/' + file_path[0].split('/')[-1] + '-semantic-removal' + file_path[1]\n",
    "df_improved.to_csv(output_path, sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
